---
title: "Prepping Dwell Time Data"
author: "Jessica Kosie"
date: "April 13, 2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

library(tidyverse); library(here); library(assertthat); library(lme4); library(lmerTest); library(nortest); library(DT)

source("helper/file_read_helper.R")

# make sure summary() uses Type III sums of squares
afex::set_sum_contrasts()

# turn off scientific notation
options(scipen = 999)
```
## Read in data files from MATLAB
```{r read matlab data}
data_path <- "data/matlab_data_files/"
file_list <- dir(data_path)

dt_data <- map_df(file_list, clean_participant_file) 
```
## Remove extraneous participants and data
```{r prep dwell time data}
#first, remove all "fieldclouds" slides (filler slides between trials)
dt_data <- dt_data %>% 
  filter(slidenumber != 1 & slidenumber != 22 & slidenumber != 43)

#remove KA06 for developmental delay and DD95 who was accidentally run twice
dt_data <- dt_data %>% 
  filter(subid != "KA06" & subid != "DD95")

```
## Add subject info and remove participants younger than 2y6m and older than 4y6m (only collected these data because this was tacked on to another study; never intended to use the data)
```{r}
subs <- read_csv(here('data', 'edited_subject_info.csv'))

dt_data <- dt_data %>% 
  left_join(subs) %>% 
  filter(age_months <= 54 & age_months >= 30)

```
## Walk through "analyzing dwell time data" steps.
```{r analysis steps}
#is the dwell time data normally distributed? (usually no)
hist(dt_data$rawDT)
lillie.test(dt_data$rawDT) #significant p-value means data is skewed (sig. differs from normal)

#log transform data
dt_data <- dt_data %>% 
  mutate(milliDT = rawDT * 1000, #convert to milliseconds
         logDT = log10(milliDT)) # log transform

#check normality now
hist(dt_data$logDT)
lillie.test(dt_data$logDT) #significant p-value means data is skewed (sig. differs from normal)

#check for outliers
#get means and standard deviations
meanDT <- mean(dt_data$logDT, na.rm = TRUE)
sdDT <- sd(dt_data$logDT, na.rm = TRUE)

#get the value of the mean plus 3 standard deviations (looking for outlier over this value)
highDT <- meanDT + (3*sdDT)
lowDT <- meanDT - (3*sdDT)

#create a column indicating outliers
dt_data <- dt_data %>% 
  mutate(highOutlier = case_when(logDT >= highDT ~ 1,
                                 TRUE ~ 0),
         lowOutlier = case_when(logDT <= lowDT ~ 1,
                                TRUE ~ 0),
         anyOutlier = case_when(logDT >= highDT | logDT <= lowDT ~ 1,
                                TRUE ~ 0))

#check whether any one participant has more than 10% outliers
outlier_summary <- dt_data %>% 
  group_by(subid) %>% 
  summarise(totOutliers = sum(anyOutlier),
            n = n(),
            propOutliers = totOutliers/n)

outlier_summary %>% 
  filter(propOutliers > .10) %>% 
  datatable()

#add a column to original data with info about outliers
dt_data <- dt_data %>% 
  left_join(outlier_summary[,c("subid", "propOutliers")]) %>% 
  filter(propOutliers <= .10) #remove participants with > 10% outlying data

#how much of the data were outliers after removing individuals with >10% outlying data?
dt_data %>% 
  summarise(tot_highOutliers = sum(highOutlier)) %>% 
  mutate(propOutliers = tot_highOutliers / nrow(dt_data))


#winsorize the data
dt_data <- dt_data %>% 
  mutate(logDT_win = case_when(logDT >= highDT ~ highDT,
                              logDT <= lowDT ~ lowDT,
                              TRUE ~ logDT)) %>% 
  dplyr::select(-contains("outlier")) #remove outlier columns


#now ask how many subjects are in each condition
dt_data %>% 
  distinct(subid, .keep_all = TRUE) %>% 
  count(condition) %>% 
  datatable()

```
# Read in pixel and slide type filse and join with dwell time file
```{r add files}
slide_types <- read_csv(here("data", "slide_types.csv"))
dt_data <- left_join(dt_data, slide_types)

pixel <- read_csv("data/pixel_change_data.csv")
dt_data <- left_join(dt_data, pixel)

```
# Remove first and last slide from analyses 
```{r remove first last slide}
dt_data <- dt_data %>% 
  filter(slide_num_adj >= 2 & slide_num_adj <= 19)

```
## Decide whether I want to residualize - do kids show a power curve (speeding up over time)?
```{r power curve decision}
dt_data %>%
  group_by(subid) %>%
  ggplot(aes(x = slide_num_adj, y = logDT_win, col = run)) +
  geom_line() +
  facet_wrap(~subid)

#are mean dwell times longer on first run?
dt_data %>% 
  group_by(run) %>% 
  summarise(mean = mean(logDT_win, na.rm = TRUE))

#nobody really shows a power curve, stopping at log transformation seems appropriate

```
## Save data file for other analyses.
```{r save file}
write_csv(dt_data, here('data', 'cleaned_dt_data.csv'))

```